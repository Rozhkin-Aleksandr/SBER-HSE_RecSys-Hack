{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93b71d-7e7a-4aff-b2cd-f7fda0bf14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from tqdm.notebook import trange\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "import torch.optim as optim\n",
    "\n",
    "train_zvuk = pd.read_parquet('train_zvuk.parquet')\n",
    "\n",
    "train_smm = pd.read_parquet('train_smm.parquet')\n",
    "\n",
    "test_smm = pd.read_parquet('test_smm.parquet')\n",
    "\n",
    "test_zvuk = pd.read_parquet('test_zvuk.parquet')\n",
    "\n",
    "\n",
    "def pivot_csr(df):\n",
    "    row_indices = pd.factorize(df['user_id'])[0]\n",
    "    col_indices = pd.factorize(df['item_id'])[0]\n",
    "    \n",
    "    # Создаем CSR-матрицу\n",
    "    csr_result = csr_matrix((df['rating'], (row_indices, col_indices)), \n",
    "                         shape=(len(df['user_id'].unique()), len(df['item_id'].unique())))\n",
    "    return csr_result\n",
    "    \n",
    "def model_1(test_zvuk, train_zvuk,train_smm, test_smm):\n",
    "    u_ids_zz = set(test_zvuk.user_id.unique())\n",
    "    u_ids_ss = set(test_smm.user_id.unique())\n",
    "    US_ID = u_ids_ss.intersection(u_ids_zz)\n",
    "    \n",
    "    train_z = test_zvuk[test_zvuk['user_id'].isin(US_ID)].sort_values(by='user_id', ascending=False)\n",
    "    train_s = test_smm[test_smm['user_id'].isin(US_ID) ].sort_values(by='user_id', ascending=False)\n",
    "    RET_Z, RET_S, RET_Z_i, RET_S_i = train_z.user_id.unique(), train_s.user_id.unique(), train_z.item_id.unique(), train_s.item_id.unique()\n",
    "    train_z_csr = pivot_csr(train_z)\n",
    "    train_s_csr = pivot_csr(train_s)\n",
    "    user_factors_z = train_z_csr\n",
    "    item_factors_z = train_z_csr.transpose()\n",
    "    user_factors_s = train_s_csr\n",
    "    item_factors_s = train_s_csr.transpose()\n",
    "    df_z = train_z_csr\n",
    "    df_s = train_s_csr\n",
    "\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for user in range(df_z.shape[0]):\n",
    "        for item in range(df_z.shape[1]):\n",
    "            if df_z[user, item] > 0: \n",
    "                X_train.append(np.concatenate((user_factors_z[user].toarray()[0],item_factors_z[item].toarray()[0], item_factors_s[item].toarray()[0],user_factors_s[user].toarray()[0]), axis=0))  # Добавляем в X_train вектор пользователя\n",
    "                y_train.append(np.array([df_z[user, item],df_s[user, item]]))  # Добавляем значение в y_train\n",
    "            if len(X_train)>=10000:\n",
    "                break\n",
    "        if len(X_train)>=10000:\n",
    "                break\n",
    "    X_train = torch.tensor(np.array(X_train)).float()\n",
    "    y_train = torch.tensor(np.array(y_train)).float()\n",
    "    mean = y_train.mean()\n",
    "    std = y_train.std()\n",
    "    # Стандартизация\n",
    "    y_train = (y_train - mean) / std\n",
    "    \n",
    "    class FullyConnectedNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(FullyConnectedNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(X_train.shape[1], 1000)\n",
    "            self.fc2 = nn.Linear(1000, 1000)\n",
    "            self.fc3 = nn.Linear(1000, 2)\n",
    "            self.relu = nn.ReLU()\n",
    "    \n",
    "        def forward(self, x):\n",
    "                x = self.relu(self.fc1(x))\n",
    "                x = self.fc3(x)  # Выходной слой без активации\n",
    "                return x\n",
    "    # Инициализация модели, функции потерь и оптимизатора\n",
    "    model_nn_z1 = FullyConnectedNN()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_nn_z1.parameters(), lr=0.001)\n",
    "    \n",
    "    # Пример данных\n",
    "    # Замените на ваши данные\n",
    "    \n",
    "    # Цикл обучения\n",
    "    num_epochs = 5  # Количество эпох\n",
    "    batch_size = 32\n",
    "    LOSS=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        l_s=0\n",
    "        for i in range(0, len(y_train), batch_size):\n",
    "            model_nn_z1.train()  # Установка модели в режим обучения\n",
    "        \n",
    "            # Прямой проход\n",
    "            outputs = model_nn_z1(X_train[i:i+batch_size])\n",
    "            loss = criterion(outputs, y_train[i:i+batch_size])\n",
    "            \n",
    "            # Обратный проход и оптимизация\n",
    "            optimizer.zero_grad()  # Обнуление градиентов\n",
    "            loss.backward()        # Вычисление градиентов\n",
    "            optimizer.step()       # Обновление весов\n",
    "        \n",
    "            # Вывод информации о ходе обучения\n",
    "            l_s+=loss*batch_size/len(y_train)\n",
    "        LOSS.append(l_s.detach().numpy())\n",
    "    return model_nn_z1, RET_Z, RET_S,RET_Z_i, RET_S_i, user_factors_z, item_factors_z, user_factors_s, item_factors_s\n",
    "\n",
    "def model_2(test_zvuk, train_zvuk,train_smm, test_smm):\n",
    "    u_ids_z = set(test_zvuk.user_id.unique())\n",
    "    u_ids_s = set(test_smm.user_id.unique())\n",
    "    US_ID = u_ids_z-u_ids_s\n",
    "    train_z = test_zvuk[test_zvuk['user_id'].isin(US_ID)].sort_values(by='user_id', ascending=False)\n",
    "    train_z_csr = pivot_csr(train_z)\n",
    "    df_z = train_z_csr\n",
    "    user_factors_z = train_z_csr\n",
    "    item_factors_z = train_z_csr.transpose()\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    RET_Z, RET_Z_i = train_z.user_id.unique(), train_z.item_id.unique()\n",
    "    for user in range(df_z.shape[0]):\n",
    "        for item in range(df_z.shape[1]):\n",
    "            if df_z[user, item] > 0:  # Проверяем наличие взаимодействия\n",
    "                X_train.append(np.concatenate((user_factors_z[user].toarray()[0],item_factors_z[item].toarray()[0]), axis=0))  # Добавляем в X_train вектор пользователя\n",
    "                y_train.append(df_z[user, item])  # Добавляем значение в y_train\n",
    "            if len(X_train)>=10000:\n",
    "                break\n",
    "        if len(X_train)>=10000:\n",
    "                break\n",
    "    X_train = torch.tensor(np.array(X_train)).float()\n",
    "\n",
    "    y_train = torch.tensor(np.array(y_train)).float()\n",
    "    \n",
    "    mean = y_train.mean()\n",
    "    std = y_train.std()\n",
    "    \n",
    "    # Стандартизация\n",
    "    y_train = (y_train - mean) / std\n",
    "    class FullyConnectedNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(FullyConnectedNN, self).__init__()\n",
    "            self.fc1 = nn.Linear(X_train.shape[1], 1000)\n",
    "            self.fc2 = nn.Linear(1000, 1000)\n",
    "            self.fc3 = nn.Linear(1000, 1)\n",
    "            self.relu = nn.ReLU()\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.fc3(x)  # Выходной слой без активации\n",
    "            return x\n",
    "    \n",
    "    # Инициализация модели, функции потерь и оптимизатора\n",
    "    model_nn_z1 = FullyConnectedNN()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model_nn_z1.parameters(), lr=0.001)\n",
    "    \n",
    "    # Пример данных\n",
    "    # Замените на ваши данные\n",
    "    \n",
    "    # Цикл обучения\n",
    "    num_epochs = 5  # Количество эпох\n",
    "    batch_size = 32\n",
    "    LOSS=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        l_s=0\n",
    "        for i in range(0, len(y_train), batch_size):\n",
    "            model_nn_z1.train()  # Установка модели в режим обучения\n",
    "        \n",
    "            # Прямой проход\n",
    "            outputs = model_nn_z1(X_train[i:i+batch_size])\n",
    "            loss = criterion(outputs.flatten(), y_train[i:i+batch_size])\n",
    "        \n",
    "            # Обратный проход и оптимизация\n",
    "            optimizer.zero_grad()  # Обнуление градиентов\n",
    "            loss.backward()        # Вычисление градиентов\n",
    "            optimizer.step()       # Обновление весов\n",
    "        \n",
    "            # Вывод информации о ходе обучения\n",
    "            l_s+=loss*batch_size/len(y_train)\n",
    "        LOSS.append(l_s.detach().numpy())\n",
    "    return model_nn_z1, RET_Z, RET_Z_i, user_factors_z, item_factors_z\n",
    "\n",
    "def test_intersect(test_zvuk, train_zvuk,train_smm, test_smm, RET_Z, RET_S,RET_Z_i, RET_S_i, model_1):\n",
    "    prediction_zvuk = {}\n",
    "    prediction_smm = {}\n",
    "    \n",
    "    for ii in range(len(RET_Z)):\n",
    "        preds =torch.tensor([])\n",
    "        preds2 =torch.tensor([]) \n",
    "        for jj in range(10000):\n",
    "            preds = torch.cat((preds, torch.tensor([model_1(torch.tensor(np.concatenate((user_factors_z[ii].toarray()[0],item_factors_z[jj].toarray()[0], item_factors_s[jj].toarray()[0],user_factors_s[ii].toarray()[0]))).float())[0]])))\n",
    "        for jj in range(10000):\n",
    "            preds2= torch.cat((preds2, torch.tensor([model_1(torch.tensor(np.concatenate((user_factors_z[ii].toarray()[0],item_factors_z[jj].toarray()[0], item_factors_s[jj].toarray()[0],user_factors_s[ii].toarray()[0]))).float())[1]])))\n",
    "        preds_ij = torch.sort(preds)[1][:10].detach().numpy()\n",
    "        preds_iijj = torch.sort(preds2)[1][:10].detach().numpy()\n",
    "        for k in range(len(preds_ij)):\n",
    "            preds_ij[k] = RET_Z_i[preds_ij[k]]\n",
    "        for k in range(len(preds_iijj)):\n",
    "            preds_iijj[k] = RET_S_i[preds_iijj[k]]\n",
    "        prediction_smm[RET_Z[ii]] = preds_iijj\n",
    "        prediction_zvuk[RET_Z[ii]] =preds_ij\n",
    "    df_data1 = {\n",
    "    'index': range(len(prediction_zvuk)),\n",
    "    'user_id': [],\n",
    "    'item_ids': []\n",
    "    }\n",
    "    for key, value in prediction_zvuk.items():\n",
    "        df_data1['user_id'].append(key)  \n",
    "        df_data1['item_ids'].append(value)\n",
    "    df1 = pd.DataFrame(df_data1)\n",
    "    df_data2 = {\n",
    "    'index': range(len(prediction_smm)),\n",
    "    'user_id': [],\n",
    "    'item_ids': []\n",
    "    }\n",
    "    for key, value in prediction_smm.items():\n",
    "        df_data2['user_id'].append(key)  \n",
    "        df_data2['item_ids'].append(value)\n",
    "    df2 = pd.DataFrame(df_data2)\n",
    "    return df1, df2\n",
    "    \n",
    "def test_z(test_zvuk, train_zvuk,train_smm, test_smm, RET_Z, RET_S,RET_Z_i, RET_S_i, model_111):\n",
    "    prediction_zvuk = {}    \n",
    "    for ii in range(RET_Z):\n",
    "        preds =torch.tensor([])\n",
    "        for jj in range(10000):\n",
    "            preds = torch.cat((preds, torch.tensor([model_111(torch.tensor(np.concatenate((user_factors_z[ii].toarray()[0],item_factors_z[jj].toarray()[0]))).float())])))\n",
    "        preds_ij = torch.sort(preds)[1][:10].detach().numpy()\n",
    "        for k in range(len(preds_ij)):\n",
    "            preds_ij[k] = RET_Z_i[preds_ij[k]]\n",
    "        prediction_zvuk[RET_Z[ii]] =preds_ij\n",
    "    df_data = {\n",
    "    'index': range(len(prediction_zvuk)),\n",
    "    'user_id': [],\n",
    "    'item_ids': []\n",
    "    }\n",
    "    for key, value in prediction_zvuk.items():\n",
    "        df_data['user_id'].append(key)  \n",
    "        df_data['item_ids'].append(value)\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "model_nn_z1, RET_Z, RET_S,RET_Z_i, RET_S_i, user_factors_z, item_factors_z, user_factors_s, item_factors_s = model_1(test_zvuk, train_zvuk,train_smm, test_smm)\n",
    "\n",
    "\n",
    "\n",
    "z1, s1 =test_intersect(test_zvuk, train_zvuk,train_smm, test_smm, RET_Z, RET_S,RET_Z_i, RET_S_i, model_nn_z1)\n",
    "\n",
    "model_nn_z2, RET_Z, RET_Z_i, user_factors_z, item_factors_z =  model_2(test_zvuk, train_zvuk,train_smm, test_smm)\n",
    "\n",
    "z2 = test_z(test_zvuk, train_zvuk,train_smm, test_smm, RET_Z, RET_S,RET_Z_i, RET_S_i, model_nn_z2)\n",
    "\n",
    "model_nn_z3, RET_Z, RET_Z_i, user_factors_z, item_factors_z =  model_2(test_smm, train_zvuk,train_smm, test_zvuk)\n",
    "\n",
    "s2 = test_z(test_smm, train_zvuk,train_smm, test_zvuk, RET_Z, RET_S,RET_Z_i, RET_S_i, model_nn_z3)\n",
    "\n",
    "Z = pd.concat([z1,z2], ignore_index=True)\n",
    "Z['index'] = Z.index\n",
    "S = pd.concat([s1,s2], ignore_index = True)\n",
    "S['index'] = S.index\n",
    "\n",
    "Z.to_parquet('submission_zvuk.parquet', index = False)\n",
    "S.to_parquet('submussion_smm.parquet', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21ffcf-74c7-4ef9-8745-24c639e69c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
